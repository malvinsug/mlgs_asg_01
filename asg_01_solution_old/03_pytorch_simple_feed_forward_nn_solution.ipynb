{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning - SS20\n",
    "## Tutorial 03 - Simple feed forward neural network\n",
    "\n",
    "* Lecturer: Prof. Dr. Volker Tresp\n",
    "* Assistants: Christian Frey, Daniyal Kazempour\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will learn how to set up a simple feed forward neural network for predicting classes of handwritten digits. Therefore, we will use the MNIST dataset containing 60.000 samples in the training set and 10.000 examples in the test data. One image of MNIST has a size of 28x28 pixel.\n",
    "To get more information about the MNISTS Database, please refer to : http://yann.lecun.com/exdb/mnist/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading MNIST Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "train_data = dsets.MNIST(root=\"./data\", train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_data = dsets.MNIST(root=\"./data\", train=False, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define batch size and number of epochs\n",
    "\n",
    "* **Minibatches.** The MNIST dataset contains 60.000 training samples. Therefore we want to split them up to smaller groups, called mini-batches. Later on, we will pass one at a time to the neural network on which it can learn on. \n",
    "* **Iterations.** We call the process of training on one minibatch an *iteration* (i.e. one weight update). Therefore, having 60.000 images and a mini-batch size of 100, we would have 600 iterations.\n",
    "* **Epoch.** An epoch means that the whole training set has been used for training the neural network. That means, if we want to learn on the whole dataset for 10 epochs, then we have in total 10*600 = 6000 iterations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For that purpose will first define the variables *batch_size*, *num_epochs* and *n_iters* indicating the size of the minibatches, the number of epochs and the number of iterations where the latter is dependent on the first two variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "num_epochs = 10\n",
    "n_iters = int(len(train_data)*num_epochs/batch_size)\n",
    "print(n_iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataLoader in PyTorch\n",
    "PyTorch provides some very strong tools on handling loading data and preprocessing data. The MNIST dataset set having been loaded above is of type *torch.utils.data.Dataset*. *Dataset* in PyTorch is an abstract class representing a dataset. Therefore, whenever you create a custom dataset it should inherit from *Dataset* and provide the 2 methods:\n",
    "* \\_\\_len\\_\\_: returns the size of the dataset\n",
    "* \\_\\_getitem\\_\\_: supporting indexings such that data[i] can be used to receive the i-th sample from the dataset\n",
    "\n",
    "A *DataLoader* can then be used as an iterator for the dataset. For this introductory example it is sufficient to know that we can tell the DataLoader the minibatch size we would like to have and that we want the samples to be reshuffled at every epoch (For further details on the DataLoader parameters, please have a look at the API).\n",
    "\n",
    "Therefore, we will no create 2 DataLoaders, namely *train\\_loader*, *test\\_loader*, where the former one is a dataloader for the training data and the latter one for the test data. We provide for each of them the size of the minibatches having been calculated above and that we also want to reshuffle the training data for each epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader for training set\n",
    "train_load = torch.utils.data.DataLoader(dataset = train_data, batch_size=batch_size, shuffle=True)\n",
    "# dataloader for test set\n",
    "test_load = torch.utils.data.DataLoader(dataset=test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create FeedForward Neural Network\n",
    "Next, we will create a class for our first neural network model. As already seen in the last tutorial, we simply have to define the modules we would like to have in our model and we have to provide a *forward* function. \n",
    "\n",
    "We will start with a very simple model consisting of three layers, an input layer, an hidden layer and an output layer. Hence, we use two linear modules to define the linear combination from the input layer to the hidden layer, and from the hidden layer to the output layer. For the activation function of the hidden layer we use the sigmoid function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedforwardNN (nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        # first module\n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim)\n",
    "        # activation function\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        # second module\n",
    "        self.linear2 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return self.linear2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate model\n",
    "The input dimension is clearly defined by the size of the images of the MNIST datset. One image is of size 28x28 pixel making the input dimension 784. The dimension of the ouput layer is defined by the classes of the MNIST dataset. Test and play around with the dimension of the hidden layer to see how it improves or downgrades your model. \n",
    "\n",
    "Instantiate the model and attach the dimensions for each layer as parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 28*28\n",
    "hidden_dim = 100\n",
    "output_dim = 10\n",
    "model = FeedforwardNN(input_dim, hidden_dim, output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As loss function we will use the cross entropy cost function having been introduces in the lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Loss Function\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As *optimizer* we will use again the stochastic gradient descent optimizer from the last notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Optimizer\n",
    "learning_rate = 1e-1\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train it\n",
    "Next, we will train our model in the same manner as we have learned it in the previous notebook. The logic for the training procedure is as follows: \n",
    "* We learn the model for $n$ epochs\n",
    "* In each epoch, we use the training dataloader to iterate the training data to get its minibatches\n",
    "* Next, we execute the learning procedure (same as has already been shown in the previous notebook\n",
    "* In order to get a feedback from the sytem on how good our model is learning we will also calculate the accuracy of our model on the test data (note that the test data is not used for training). Therefore, after a certain number of *iterations*/*epochs* we would like to have a response about the accuracy. For calculating the accuracy, we use the dataloader on the test data, predict the classes with our model and calculate the number of right predictions with respect to the total size of the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "\tTraining loss: 0.690066397190094\n",
      "\tAccuracy on test data: 87.00\n",
      "Epoch 2:\n",
      "\tTraining loss: 0.5256693363189697\n",
      "\tAccuracy on test data: 90.00\n",
      "Epoch 3:\n",
      "\tTraining loss: 0.4824374318122864\n",
      "\tAccuracy on test data: 90.00\n",
      "Epoch 4:\n",
      "\tTraining loss: 0.4349577724933624\n",
      "\tAccuracy on test data: 91.00\n",
      "Epoch 5:\n",
      "\tTraining loss: 0.40147295594215393\n",
      "\tAccuracy on test data: 92.00\n",
      "Epoch 6:\n",
      "\tTraining loss: 0.39231643080711365\n",
      "\tAccuracy on test data: 92.00\n",
      "Epoch 7:\n",
      "\tTraining loss: 0.3937968313694\n",
      "\tAccuracy on test data: 92.00\n",
      "Epoch 8:\n",
      "\tTraining loss: 0.3628355860710144\n",
      "\tAccuracy on test data: 92.00\n",
      "Epoch 9:\n",
      "\tTraining loss: 0.3557168245315552\n",
      "\tAccuracy on test data: 93.00\n",
      "Epoch 10:\n",
      "\tTraining loss: 0.33277031779289246\n",
      "\tAccuracy on test data: 93.00\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "for epoch in range(1,num_epochs+1):\n",
    "    model.train()\n",
    "    for i , (x_mb, y_mb) in enumerate (train_load):\n",
    "        x_mb = x_mb.view(-1, 28*28)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x_mb)\n",
    "        loss = criterion(y_pred, y_mb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation after n epochs (here: after each epoch)\n",
    "    if epoch % 1 == 0:\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0 \n",
    "        test_mb_loss = []\n",
    "        for x_test, y_test in test_load:\n",
    "            x_test = x_test.view(-1,28*28)\n",
    "            y_pred=model(x_test)\n",
    "            loss = criterion(y_pred, y_test)\n",
    "            _, max_indices = torch.max(y_pred, 1)\n",
    "            total += y_test.size(0)\n",
    "            correct += (max_indices == y_test).sum()\n",
    "        acc = 100. * correct/total\n",
    "\n",
    "    print (\"Epoch {}:\\n\\tTraining loss: {}\\n\\tAccuracy on test data: {:.2f}\".format(epoch,  loss.item(), acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of this tutorial"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
